# -*- coding: utf-8 -*-
"""Copy of Copy of DS Reddit.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ipcpaSXDmaEQKEjAvcSoJjwO-vdVPyKv
"""

import requests
import pandas as pd
query="TSLA" #Define Your Query
url = f"https://api.pushshift.io/reddit/search/comment/?q={query}"
request = requests.get(url)
json_response = request.json()
#json_response
#df = pd.read_json('/tsla.json')

from json.decoder import JSONDecodeError
def get_pushshift_data(data_type, **kwargs):
    try:
      base_url = f"https://api.pushshift.io/reddit/search/{data_type}/"
      payload = kwargs
      request = requests.get(base_url, params=payload)
      return request.json()
    except JSONDecodeError:
      return None

data_type="comment"     # give me comments, use "submission" to publish something
query="TSLA"          # Add your query
duration="720d"        # Select the timeframe. Epoch value or Integer + "s,m,h,d" (i.e. "second", "minute", "hour", "day")
size=500             # maximum 1000 comments
sort_type="created_utc"       # Sort by score (Accepted: "score", "num_comments", "created_utc")
sort="desc"             # sort descending
aggs="subreddit"
get_pushshift_data(data_type=data_type,     
                   q=query,   
                   before="1609379830",              
                   after="1546307830",          
                   size=size,               
                   sort_type=sort_type,
                   sort=sort)

!pip install nltk
import pandas as pd
import nltk
nltk.download('vader_lexicon')
from nltk.sentiment.vader import SentimentIntensityAnalyzer
day = 6
new_words = {
    'yolo': 3.0,
    'bullish': 10.0,
    'moon':10.0,
    'gains':3.0,
    'hold':10.0,
    'ape':10.0,
    'tendies':10.0,
    'drop':-10.0,
    'bear':-10.0,
    'bag holding':-10.0,
    'calls':10.0,
  }
SIA = SentimentIntensityAnalyzer()
SIA.lexicon.update(new_words)
results_daily = []
for i in range (0,720):
  #get current day here
  check = False
  end_time = str(828-i)+"d"
  data_type="comment"     # give me comments, use "submission" to publish something
  query="TSLA"          # Add your query
  duration= str(829 - i)+"d"    # Select the timeframe. Epoch value or Integer + "s,m,h,d" (i.e. "second", "minute", "hour", "day")
  size=1000             
  sort_type="created_utc"       # Sort by score (Accepted: "score", "num_comments", "created_utc")
  sort="desc"             # sort descending
  aggs="subreddit"
  if day +1 == i +2:
    #print("skip2")
    
    day = day + 7
    
    continue
  if day == i +2:
    #print("skip")
    
    #day = day + 7
    #print(i)
    continue
  
  data = get_pushshift_data(data_type=data_type,
                          q=query,
                          before=end_time,
                          after=duration,
                          size=size,
                          aggs=aggs)
  
  if data == None:
    results_daily.append(0)
    continue
  if(len(data.values())==0):
    results_daily.append(0)
    continue
  data = list(data.values())[0]
  
    
  #print(data)
  df = pd.DataFrame.from_records(data)
  #print(df["created_utc"][0])
  #print(i+2)
  #print(df)
  if df.empty:
    results_daily.append(0)
    continue
  
  df = pd.DataFrame.from_records(data)[["author", "subreddit", "score", "body", "permalink"]]
#df
  

  
  titles = df['body']

  results = []
  sum = 0.0
  for title in titles:
      pol_score = SIA.polarity_scores(title)
      pol_score['title'] = title
      sum += pol_score['compound']
      results.append(pol_score)
      #print(pol_score)
    #print(title)
  
  avg_day = sum / len(titles)
  print(avg_day)
  results_daily.append(avg_day)
  

print(results_daily)

!pip install scipy
from scipy import stats as sp
import numpy as np
import matplotlib.pyplot as plt
dfsp = pd.read_json('stockprice.json')
#print(dfsp['Change %'])
price_changes = dfsp['Change %'].to_numpy()
#print(price_changes)
def f(x):
  #print(x)
  xsize = len(x)
  for index in range(0,xsize):
    xwsize = len(x[index])-1
    x[index]=float(x[index][0:xwsize])
  
  return x

pc = f(price_changes)
pct_change_res = []
rd_size = len(results_daily)
print(rd_size)
for i in range(0,(rd_size-1)):
  num1 = results_daily[i+1]
  num2 = results_daily[i]
  if num1==0:
    num1 = 0.01
  pct_change_res.append((num1 -num2)/ num1)
pc = pc[0:504]
pc=np.array(pc).reshape(1,-1)
pct_change_res=np.array(pct_change_res).reshape(1,-1).astype(float)
#print(len(pct_change_res))

npscopy1 = np.copy(pc)
npsortcopy1 = np.sort(npscopy1)
#print(npsortcopy)
npsortcopy21 = np.flip(npsortcopy1)
newmin = npsortcopy1[0][1]
newmax = npsortcopy21[0][1]
print((newmin))
print((newmax))
diff = (newmax-newmin)
print(diff)
pc = pc * (100 /diff)

#print(pct_change_res)
#print("b+bmax")
#print(b+bmax)

npscopy = np.copy(pct_change_res)
npsortcopy = np.sort(npscopy)
#print(npsortcopy)
npsortcopy2 = np.flip(npsortcopy)
newmin = npsortcopy[0][1]
newmax = npsortcopy2[0][1]
print((newmin))
print((newmax))
diff = (newmax-newmin)
print(diff)
pct_change_res = pct_change_res * (100 /diff)
#print(pct_change_res)
#print(pct_change_res)
print(pc)
pc=pc.astype(float)
slope, intercept, r_value, p_value, std_err = sp.linregress(pc[0][1:], pct_change_res)
print(r_value**2)
#plt.plot(pct_change_res, pc[:505], 'o')

